{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52fbe5bb",
   "metadata": {},
   "source": [
    "# 用 BERT进行实体抽取\n",
    "\n",
    "在知识图谱中，实体抽取是一个基本任务，在珠峰书《知识图谱：认知智能理论与实战》第3章（P78~136）详细介绍了各种实体抽取的方法，本文介绍基于 BERT的实体抽取方法。\n",
    "\n",
    "BERT 是英文“Bidirectional Encoder Representations from Transformers”的缩写，是一个大规模预训练模型，它通过对数十亿个词所组成的语料进行预训练而形成强大的基础语义，同时通过精心设计的掩码语言模型（Masked Language Model，MLM）来模拟人类对语言的认知，形成了效果卓绝的模型。在2020 年的一篇论文*《A Primer in BERTology: What We Know About How BERT Works》*中提到：\n",
    "```text\n",
    "In a little over a year, BERT has become a ubiquitous baseline in NLP experiments and inspired numerous studies analyzing the model and proposing various improvements. The stream of papers seems to be accelerating rather than slowing down, and we hope that this survey helps the community to focus on the biggest unresolved questions.\n",
    "```\n",
    "自从 BERT 出来以后，也引导了至今炙手可热的“大模型”浪潮。其本质就是“预训练”+“微调”的模式，而这一切都是自BERT而始的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e26154cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9864705d",
   "metadata": {},
   "source": [
    "在珠峰书《知识图谱》中，使用了 PyTorch和transformers包来作为示例来说明如何使用 BERT 进行实体抽取。为了统一，本例子则使用了飞桨框架来实现。特别的，本例中使用了paddlenlp包中提供了与transformers模块类似的功能的模块。\n",
    "\n",
    "有关 paddlenlp 的内容，可参考官方文档：\n",
    "- https://paddlenlp.readthedocs.io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30792b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import paddle\n",
    "from paddle import nn\n",
    "import paddlenlp\n",
    "from paddlenlp.transformers import BertTokenizer\n",
    "from paddlenlp.transformers import BertForTokenClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "170247c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.2 2.3.7\n"
     ]
    }
   ],
   "source": [
    "print(paddle.__version__, paddlenlp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57efda7",
   "metadata": {},
   "source": [
    "## 数据准备\n",
    "\n",
    "本例子使用 MSRA 发布的公开命名实体识别的语料，可以从【datasets/NER-MSRA】目录下获得处理好的数据集，包括训练语料train.txt和测试语料test.txt。语料的说明见该目录下的 readme。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b8ece4",
   "metadata": {},
   "source": [
    "### 读取数据\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50805e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  45057\n",
      "test:  3442\n"
     ]
    }
   ],
   "source": [
    "#载入数据\n",
    "\n",
    "def read_data(filename):\n",
    "    '''读入训练语料，每个句子使用 list 存储\n",
    "    格式为适合crf++的格式：\n",
    "        每行格式为 token\\t标签\n",
    "        空行表示句子结束\n",
    "    @param filename: 语料文件名\n",
    "        '''\n",
    "    data = []\n",
    "\n",
    "    sent = []\n",
    "    lbl = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                data.append((sent, lbl))\n",
    "                sent = []\n",
    "                lbl = []\n",
    "                continue\n",
    "            c, t = line.split(\"\\t\")\n",
    "            sent.append(c)\n",
    "            lbl.append(t)\n",
    "        if sent:\n",
    "            data.append((sent, lbl))\n",
    "    return data\n",
    "\n",
    "train_data = read_data('./msra/train.txt')\n",
    "print('train: ', len(train_data))\n",
    "test_data = read_data('./msra/test.txt')\n",
    "print('test: ', len(test_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbcd74a",
   "metadata": {},
   "source": [
    "### Paddle 的 Dataset \n",
    "\n",
    "将数据集转化为 Paddle 的 Dataset 格式，方便后续给模型使用\n",
    "\n",
    "`paddle.io.Dataset` 是Paddle数据集的抽象类，需要实现如下两个方法：\n",
    "\n",
    "- `__getitem__`: 根据给定索引获取数据集中指定样本，在 `paddle.io.DataLoader` 中需要使用此函数通过下标获取样本。\n",
    "\n",
    "- `__len__`: 返回数据集样本个数， `paddle.io.BatchSampler` 中需要样本个数生成下标序列。\n",
    "\n",
    "\n",
    "参考：https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/Dataset_cn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab309b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TheDataset(paddle.io.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids, token_type_ids, attention_mask, labels = self.data[idx]\n",
    "        return input_ids, token_type_ids, attention_mask, labels\n",
    "    \n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02504212",
   "metadata": {},
   "source": [
    "### 转化为 Paddle 的 Dataset\n",
    "\n",
    "这里使用了 BERT 模型，输入的文本要使用BertTokenizer来将其转化为 id\n",
    "\n",
    "在本例中使用了谷歌原始发布的 BERT 中文基础版模型bert-base-chinese，其基本信息如下：\n",
    "\n",
    "- 12-layer, 768-hidden, 12-heads, 108M parameters. \n",
    "- Trained on cased Chinese Simplified and Traditional text.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb1464d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_NAME = 'bert-base-chinese'\n",
    "max_seq_len = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2ff7f",
   "metadata": {},
   "source": [
    "将文本转化为用于训练和测试的 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baddd3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-10-04 06:30:41,576] [    INFO]\u001b[0m - Already cached /home/kgbook/.paddlenlp/models/bert-base-chinese/bert-base-chinese-vocab.txt\u001b[0m\n",
      "\u001b[32m[2022-10-04 06:30:41,595] [    INFO]\u001b[0m - tokenizer config file saved in /home/kgbook/.paddlenlp/models/bert-base-chinese/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-10-04 06:30:41,596] [    INFO]\u001b[0m - Special tokens file saved in /home/kgbook/.paddlenlp/models/bert-base-chinese/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704\n",
      "[CLS] 101\n",
      "[SEP] 102\n",
      "[PAD] 0\n"
     ]
    }
   ],
   "source": [
    "# 初始化tokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_NAME)\n",
    "print(tokenizer.convert_tokens_to_ids('中'))\n",
    "for i in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "    print(i, tokenizer.convert_tokens_to_ids(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d102ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转化为dataset\n",
    "\n",
    "label2id = {\n",
    " 'O': 0,\n",
    " 'B-LOC': 1,\n",
    " 'I-LOC': 2,\n",
    " 'B-ORG': 3,\n",
    " 'I-ORG': 4,\n",
    " 'B-PER': 5,\n",
    " 'I-PER': 6\n",
    "}\n",
    "\n",
    "\n",
    "pad_id = tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "cls_id = tokenizer.convert_tokens_to_ids('[CLS]')\n",
    "sep_id = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "unk_id = tokenizer.convert_tokens_to_ids('[UNK]')\n",
    "unword_label_id = label2id['O']\n",
    "\n",
    "def trans2TheDataset(data, max_seq_len):\n",
    "    data2 = []\n",
    "    \n",
    "    for k, v  in data:\n",
    "        k = [tokenizer.convert_tokens_to_ids(i) for i in k]\n",
    "        v = [label2id[i] for i in v]\n",
    "        \n",
    "        \n",
    "        input_ids = [cls_id] + k\n",
    "        labels = [unword_label_id] + v\n",
    "        \n",
    "        if len(input_ids) > max_seq_len-1:\n",
    "            input_ids = input_ids[:max_seq_len-1]\n",
    "            labels = labels[:max_seq_len-1]\n",
    "        \n",
    "\n",
    "        input_ids.append(sep_id)\n",
    "        labels.append(unword_label_id)\n",
    "        \n",
    "        klen = len(input_ids)\n",
    "\n",
    "        \n",
    "            \n",
    "        if klen < max_seq_len:\n",
    "            pad_len = max_seq_len-klen\n",
    "        \n",
    "            input_ids = input_ids + [pad_id] * pad_len\n",
    "            labels = labels + [unword_label_id] * pad_len\n",
    "            \n",
    "            token_type_ids = [0] * max_seq_len\n",
    "            attention_mask = [1] * klen + [0] * pad_len\n",
    "        \n",
    "        \n",
    "        data2.append((np.asarray(input_ids, dtype='int64'), \n",
    "                      np.asarray(token_type_ids, dtype='int64'),\n",
    "                      np.asarray(attention_mask, dtype='int64'),\n",
    "                      np.asarray(labels, dtype='int64')))\n",
    "    return TheDataset(data2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c650ac2",
   "metadata": {},
   "source": [
    "### 创建数据集\n",
    "\n",
    "由于 msra 数据集仅提供了 train 和 test，没有 dev 数据集。这里将 train 进行二八划分为 dev 和 train 两个数据集。\n",
    "\n",
    "另外，在输入中需要对过长的输入句子进行截断，这里设置max_seq_len为255，在实际应用中可根据情况取值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a248ab22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36046 9011\n",
      "36046 9011\n",
      "36046 9011 3442\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train_data 拆分为 train 和 dev\n",
    "train_data_count = len(train_data)\n",
    "dev_data_count = int(train_data_count * 0.2)\n",
    "train_data_count -= dev_data_count\n",
    "print(train_data_count, dev_data_count)\n",
    "dev_data = train_data[:dev_data_count]\n",
    "train_data = train_data[dev_data_count:]\n",
    "print(len(train_data), len(dev_data))\n",
    "\n",
    "train_dataset = trans2TheDataset(train_data, max_seq_len)\n",
    "dev_dataset = trans2TheDataset(dev_data, max_seq_len)\n",
    "test_dataset = trans2TheDataset(test_data, max_seq_len)\n",
    "\n",
    "print(len(train_dataset), len(dev_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3f8363",
   "metadata": {},
   "source": [
    "### 构建支持获取微批数据的 Dataloader\n",
    "\n",
    "- `DataLoader`返回一个迭代器，该迭代器根据 batch_sampler 给定的顺序迭代一次给定的 dataset\n",
    "\n",
    "- `DataLoader`支持单进程和多进程的数据加载方式，当 num_workers 大于0时，将使用多进程方式异步加载数据。\n",
    "\n",
    "参考：https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/DataLoader_cn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7be37d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "# 加载数据\n",
    "train_loader = paddle.io.DataLoader(train_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "dev_loader = paddle.io.DataLoader(dev_dataset, shuffle=False, batch_size=batch_size, drop_last=False)\n",
    "test_loader = paddle.io.DataLoader(test_dataset, shuffle=False, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5924f0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([8, 256], [8, 256], [8, 256], [8, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for b in train_loader:\n",
    "    break\n",
    "x, a, m, y = b\n",
    "x.shape, a.shape, m.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e533c484",
   "metadata": {},
   "source": [
    "## 构建BERT实体抽取模型\n",
    "\n",
    "\n",
    "在《知识图谱：认知智能理论与实战》一书中，对实体的定义为：\n",
    "\n",
    "*实体（Entity）：是指一种独立的、拥有清晰特征的、能够区别于其他事物的事物。在信息抽取、自然语言处理和知识图谱等领域，用来描述这些事物的信息即实体。实体可以是抽象的或者具体的。*\n",
    "\n",
    "                                                         ——王文广 《知识图谱：认知智能理论与实战》 P81\n",
    "\n",
    "这是对1996年MUC-6会议对命名实体的扩展。MUC组委会在当时提出的“命名实体”任务要求从文本中识别出所有的人物名称（人名）、组织机构名称（机构名）和地理位置名称（地名），以及时间、货币和百分数的表述。如果仅仅识别人名、地名、机构名等实体的话，常见的分词库（如 jieba、HanLP、LAC 等）都支持的，可以直接使用这些库来识别，效果通常还不错。\n",
    "\n",
    "而如果要在产业应用中进行实体抽取，仅仅能够处理这几个命名实体则远远不够。比如书名的识别、建筑物名称的识别、汽车品牌的识别、汽车零部件的识别等等。\n",
    "\n",
    "*在实践中，实体不一定是对物理事物的表述，也可以是对虚拟事物的表述。比如“经济指标”类型的实体“CPI”、人物或者组织机构发表的“观点”类型的实体、某个领域权威人物发表的“言论”类型的实体，在制造业质量和可靠性工程中的“失效事件”类型的实体，以及在各类机械与电子电器设备制造领域中的“性能”类型的实体等。*\n",
    "\n",
    "                                                        ——王文广 《知识图谱：认知智能理论与实战》 P81\n",
    "\n",
    "实体抽取（命名实体识别）就是从一段文本中抽取出符合要求的实体，常见的实体抽取方法非常多，在《知识图谱：认知智能理论与实战》的第三章介绍了主流的几种实体抽取方法。下面的模型来自于该书（珠峰书）3.5.3节《预训练模型用于实体抽取》，详细内容参考珠峰书《知识图谱：认知智能理论与实战》一书P122~133。\n",
    "\n",
    "值得注意的是，本文使用飞桨框架，而书中的代码示例则使用了 pytorch。\n",
    "\n",
    "关于 BERT 的模型详解，可参考珠峰书的3.5.3节《预训练模型用于实体抽取》P122~133。书中非常详细地解析了 BERT 的结构，包括掩码语言模型、多头注意力机制、位置嵌入、片段嵌入和词元嵌入等。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88db460",
   "metadata": {},
   "source": [
    "顺带值得一提的是，对于普罗大众来说，人工智能的标志性事件当属 AlphaGo，号称人类最难的智力游戏败于机器，可是街头巷尾的谈资。但在自然语言处理领域，BERT，在当时的自然语言处理领域可谓掀起轩然大波，总结起来有：\n",
    "\n",
    "- 在机器阅读理解顶级水平测试SQuAD1.1中表现出惊人成绩，首次两个衡量指标上全面超越人类，并且还在11种不同NLP测试中创出最佳成绩\n",
    "- 谷歌团队成员Thang Luong表示，BERT模型开启了NLP的新时代\n",
    "- 证明了通过大规模语料集预训练的技术，能够大幅度提升各类文本阅读理解的效果，也因此，“大模型”自此兴起\n",
    "- Masked LM通过学习masked的词，不仅让模型学会了上下文信息，还学会了语法syntax、语义semantics、语用pragmatics等，并能够很好的学会部分领域知识。\n",
    "- 预训练模型越大，效果越好；对应的，成本也会越高。相比于单任务模型来说，无监督的预训练模型成本要大1000倍以上\n",
    "- 学术界传统上认为很难处理的一些文字阅读理解任务上，计算机有望能够全面超越人类\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7dd4c6",
   "metadata": {},
   "source": [
    "关于`BertForTokenClassification`可从参考：\n",
    "- https://paddlenlp.readthedocs.io/zh/latest/source/paddlenlp.transformers.bert.modeling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75543f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 实体抽取模型\n",
    "class EEModel(nn.Layer):\n",
    "    \"\"\"用于实体抽取（命名实体识别）的BERT模型\"\"\"\n",
    "\n",
    "    def __init__(self, num_labels):\n",
    "        \"\"\"\n",
    "        @param num_labels: 标签数量\n",
    "        \"\"\"\n",
    "        super(EEModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        # 嵌入层\n",
    "        self.bert = BertForTokenClassification.from_pretrained(BERT_NAME, num_classes=num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask,token_type_ids, labels):\n",
    "        output = self.bert(input_ids, attention_mask=attention_mask,\n",
    "                           token_type_ids=token_type_ids, labels=labels,\n",
    "                           output_hidden_states=False, output_attentions=False,\n",
    "                           return_dict=False)\n",
    "        \n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66745da9",
   "metadata": {},
   "source": [
    "### 实例化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "614f1c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-10-04 06:30:54,041] [    INFO]\u001b[0m - Already cached /home/kgbook/.paddlenlp/models/bert-base-chinese/bert-base-chinese.pdparams\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1004 06:30:54.044133 90946 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.7, Runtime API Version: 11.6\n",
      "W1004 06:30:54.046743 90946 gpu_resources.cc:91] device: 0, cuDNN Version: 8.5.\n"
     ]
    }
   ],
   "source": [
    "num_labels = max([v for k, v in label2id.items()]) + 1\n",
    "print(num_labels)\n",
    "\n",
    "model = EEModel(num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f15c9",
   "metadata": {},
   "source": [
    "### 模型训练准备\n",
    "\n",
    "由于BERT 已经使用了大规模语料训练并得到了通用的语义表示，通常使用的话，仅需要重新微调最上面一层即可。为此，需要冻结其他不需要训练的层的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "066c1b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_layer = [n for n, p in model.named_parameters() if 'layers.11' in n ]\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in decay_layer)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in decay_layer)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a57a21b",
   "metadata": {},
   "source": [
    "### 使用 AdamW 优化器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce754c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00005\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=lr, parameters=optimizer_grouped_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de586676",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "\n",
    "这里使用前述语料训练30个 epoch，训练的方法是最简单的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c9c7c52",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 4505/4505 [12:02<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0 Train Acc: 0.567 Loss: 0.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 1127/1127 [01:07<00:00, 16.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0 Eval Acc: 0.518 Loss: 0.007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 4505/4505 [11:19<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 Train Acc: 0.593 Loss: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 1127/1127 [01:07<00:00, 16.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 Eval Acc: 0.523 Loss: 0.007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 4505/4505 [11:17<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2 Train Acc: 0.598 Loss: 0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 1127/1127 [01:07<00:00, 16.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2 Eval Acc: 0.519 Loss: 0.007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 4505/4505 [11:18<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3 Train Acc: 0.603 Loss: 0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 1127/1127 [01:07<00:00, 16.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 3 Eval Acc: 0.512 Loss: 0.009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 4505/4505 [11:19<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4 Train Acc: 0.605 Loss: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 1127/1127 [01:07<00:00, 16.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4 Eval Acc: 0.507 Loss: 0.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 4505/4505 [11:18<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5 Train Acc: 0.607 Loss: 0.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 1127/1127 [01:07<00:00, 16.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 5 Eval Acc: 0.501 Loss: 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 4505/4505 [11:18<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6 Train Acc: 0.608 Loss: 0.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 1127/1127 [01:07<00:00, 16.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 6 Eval Acc: 0.515 Loss: 0.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ep in range(7):\n",
    "\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "    train_count = 0\n",
    "\n",
    "    model.train()\n",
    "    for input_ids, token_type_ids, attention_mask, labels  in tqdm(train_loader):\n",
    "        optimizer.clear_grad()\n",
    "        loss, logits = model(input_ids, attention_mask,token_type_ids, labels)\n",
    "        \n",
    "        for i in range(logits.shape[0]):\n",
    "            logits_clean = logits[i][labels[i] != unword_label_id]\n",
    "            preds = logits_clean.argmax(axis=1)\n",
    "            label_clean = labels[i][labels[i] != unword_label_id]\n",
    "            acc = (preds == label_clean).cast('float').mean()\n",
    "        \n",
    "            total_acc_train += acc.item()\n",
    "            total_loss_train += loss.item()\n",
    "            train_count += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Ep {ep} Train Acc: {total_acc_train/train_count:.3f} Loss: {total_loss_train/train_count:.3f}')\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_acc_val = 0\n",
    "    total_loss_val = 0\n",
    "    dev_count = 0\n",
    "\n",
    "    with paddle.no_grad():\n",
    "        for input_ids, token_type_ids, attention_mask, labels in tqdm(dev_loader):\n",
    "            loss, logits = model(input_ids, attention_mask,token_type_ids, labels)\n",
    "\n",
    "            for i in range(logits.shape[0]):\n",
    "                logits_clean = logits[i][labels[i] != unword_label_id]\n",
    "                preds = logits_clean.argmax(axis=1)\n",
    "                label_clean = labels[i][labels[i] != unword_label_id]\n",
    "                acc = (preds == label_clean).cast('float').mean()\n",
    "                total_acc_val += acc.item()\n",
    "                total_loss_val += loss.item()\n",
    "                dev_count += 1\n",
    "            \n",
    "    print(f'Ep {ep} Eval Acc: {total_acc_val/dev_count:.3f} Loss: {total_loss_val/dev_count:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a690fd3",
   "metadata": {},
   "source": [
    "## 使用测试集评估效果\n",
    "\n",
    "在文章中（https://mp.weixin.qq.com/s/STS8N1PBML_2BvkO5NfiXg ），详细介绍了模型效果的评估，有两种方法：\n",
    "\n",
    "- 基于词元的效果评估\n",
    "- 基于实体的效果评估\n",
    "\n",
    "这里采用`基于实体的效果评估`，详情见上述文章的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2941ea3d",
   "metadata": {},
   "source": [
    "### 使用模型预测测试集的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14d7e69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 431/431 [00:25<00:00, 16.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.539 Loss: 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "  \n",
    "total_acc = 0\n",
    "total_loss = 0\n",
    "test_count = 0\n",
    "\n",
    "all_inputs = []\n",
    "all_logits = []\n",
    "all_labels = []\n",
    "all_masks = []\n",
    "\n",
    "with paddle.no_grad():\n",
    "    for input_ids, token_type_ids, attention_mask, labels in tqdm(test_loader):\n",
    "\n",
    "        all_inputs.append(input_ids)\n",
    "        all_labels.append(labels)\n",
    "        all_masks.append(attention_mask)\n",
    "        loss, logits = model(input_ids, attention_mask,token_type_ids, labels)\n",
    "        logits_cpu = logits.detach().numpy()\n",
    "        all_logits.append(logits_cpu)\n",
    "\n",
    "        for i in range(logits.shape[0]):\n",
    "            logits_clean = logits[i][labels[i] != unword_label_id]\n",
    "            preds = logits_clean.argmax(axis=1)\n",
    "            label_clean = labels[i][labels[i] != unword_label_id]\n",
    "            acc = (preds == label_clean).cast('float').mean()\n",
    "            total_acc += acc.item()\n",
    "            total_loss += loss.item()\n",
    "            test_count += 1\n",
    "\n",
    "print(f'Test Acc: {total_acc/test_count:.3f} Loss: {total_loss/test_count:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff69022",
   "metadata": {},
   "source": [
    "### 提取实体\n",
    "\n",
    "从结果中提取实体的方法，相见文章：\n",
    "- https://mp.weixin.qq.com/s/STS8N1PBML_2BvkO5NfiXg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "152cf752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entities_of_sentence(sent, labels, seq_len=None, sentid=-1):\n",
    "    '''适用于BIO标记方法'''\n",
    "\n",
    "    if type(sent) == str:\n",
    "        sent = sent.split()\n",
    "    if type(labels) == str:\n",
    "        labels = labels.split()\n",
    "    if seq_len is None:\n",
    "        seq_len = len(sent) \n",
    "      \n",
    "    entities = []\n",
    "    tokens_of_entity = []\n",
    "    type_of_entity = None\n",
    "    idx = 0\n",
    "    while idx < seq_len:\n",
    "        label = labels[idx]\n",
    "        word = sent[idx]\n",
    "        idx += 1\n",
    "        \n",
    "        if label == 'O':\n",
    "            continue\n",
    "        if label.startswith('B'):\n",
    "            # print(tokens_of_entity, type_of_entity)\n",
    "            if tokens_of_entity:\n",
    "                entities.append((sentid, idx, ''.join(tokens_of_entity), type_of_entity))\n",
    "            tokens_of_entity = [word]\n",
    "            # B-type, 比如B-ORG表示ORG类型\n",
    "            type_of_entity = label[2:]\n",
    "            continue\n",
    "        if label.startswith('I'):\n",
    "            # I-type, 比如I-ORG表示ORG类型\n",
    "            if label[2:] != type_of_entity:\n",
    "                # B-type 和 I-type不同，说明抽取结果有误\n",
    "                # 删除该抽取结果\n",
    "                tokens_of_entity = []\n",
    "                type_of_entity = None\n",
    "            else:\n",
    "                tokens_of_entity.append(word)\n",
    "    if tokens_of_entity:\n",
    "        entities.append((sentid, idx, ''.join(tokens_of_entity), type_of_entity))\n",
    "    return entities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ec12571",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {v:k for k, v in label2id.items()}\n",
    "\n",
    "\n",
    "all_ent_pred = []\n",
    "all_ent_label = []\n",
    "sent_count = 0\n",
    "for input_ids, masks, logits, labels in zip(all_inputs, all_masks, all_logits, all_labels):\n",
    "    seq_lens = masks.sum(1).numpy()\n",
    "    preds = logits.argmax(axis=2)\n",
    "    labels = labels.numpy()\n",
    "    \n",
    "    for sent, seq_len, pred, label in zip(input_ids,seq_lens, preds, labels):\n",
    "        sent_count += 1\n",
    "        sent = tokenizer.convert_ids_to_tokens(sent)[:seq_len]\n",
    "        pred = [id2label[i] for i in pred[:seq_len]]\n",
    "        label = [id2label[i] for i in label[:seq_len]]\n",
    "        all_ent_pred.extend(entities_of_sentence(sent, pred, sentid=sent_count))\n",
    "        all_ent_label.extend(entities_of_sentence(sent, label, sentid=sent_count))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1d5478",
   "metadata": {},
   "source": [
    "### 计算准确率、精确率、召回率和 F1分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b6d6191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5360 5391 4755 5996\n",
      "精确率: p=88.71268656716418\n",
      "召回率: r=88.20255982192543\n",
      "F1分数: F1=88.45688773137384\n",
      "准确率: acc=79.3028685790527\n"
     ]
    }
   ],
   "source": [
    "ee_pred = all_ent_pred\n",
    "ee_gt = all_ent_label\n",
    "\n",
    "y = len(ee_pred)\n",
    "intersect = len(set(ee_gt).intersection(set(ee_pred)))\n",
    "y_hat = len(ee_gt)\n",
    "union = len(set(ee_gt).union(set(ee_pred)))\n",
    "\n",
    "print(y, y_hat, intersect, union)\n",
    "\n",
    "\n",
    "p = intersect/y*100\n",
    "r = intersect/y_hat*100\n",
    "f1 = 2*p*r/(p+r)\n",
    "\n",
    "print('精确率: p=', p, sep='')\n",
    "print('召回率: r=', r, sep='')\n",
    "print('F1分数: F1=', f1, sep='')\n",
    "print('准确率: acc=', intersect/union*100, sep='')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd2fda",
   "metadata": {},
   "source": [
    "### 计算每个类别的 F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4782c9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG F1分数为： 82.84939992257064\n",
      "PER F1分数为： 90.51787016776076\n",
      "LOC F1分数为： 90.0847769996314\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "ee_pred = {}\n",
    "ee_gt = {}\n",
    "for sentid, idx, ent, tent in all_ent_pred:\n",
    "    if tent not in ee_pred:\n",
    "        ee_pred[tent] = []\n",
    "    ee_pred[tent].append((sentid, idx, ent))\n",
    "for sentid, idx, ent, tent in all_ent_label:\n",
    "    if tent not in ee_gt:\n",
    "        ee_gt[tent] = []\n",
    "    ee_gt[tent].append((sentid, idx, ent))\n",
    "\n",
    "\n",
    "for cate in ee_gt.keys():\n",
    "    y = set(ee_gt[cate])\n",
    "    y_hat = set(ee_pred[cate])\n",
    "    y_i = y.intersection(y_hat)\n",
    "    p, r, f1 = 0, 0, 0\n",
    "    if y_i:\n",
    "        p = len(y_i) / len(y)\n",
    "        r = len(y_i) / len(y_hat)\n",
    "        f1 = 2 * (p * r) / (p + r)\n",
    "    print(cate, 'F1分数为：', f1*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a7a332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20a854c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
